---
layout: post
title: Ensemble learning, bagging, boosting
date: '2011-08-29T15:35:00.001-04:00'
author: Xianjun Dong
tags:
- machine learning
modified_time: '2011-08-30T16:12:48.421-04:00'
blogger_id: tag:blogger.com,1999:blog-12445049.post-8554305963364181454
blogger_orig_url: http://onetipperday.blogspot.com/2011/08/ensemble-learning-bagging-boosting_29.html
---

<a href="http://en.wikipedia.org/wiki/Ensemble_learning">Ensemble learning - Wikipedia, the free encyclopedia</a>: <div>"an ensemble is a technique for combining many weak learners in an attempt to produce a strong learner"</div><div>
<br /></div><div>Bagging, abbreviation of 'bootstrap aggregating', trains each model in the ensemble using a randomly-drawn subset of the training set. So, each model is independent. Random forest algorithm, for instance, combines random decision tree with bagging to get a high classification accuracy. </div><div>
<br /></div><div>Boosting, is to build the ensembl by incrementally including new model instances to emphasize the training instances that were mis-classified by previous models. Common implementation of Boosting is Adaboost.
<br /></div><div>
<br /></div><div>Other understanding from the randomForest <a href="http://www.webchem.science.ru.nl/PRiNS/rF.pdf">documentation</a>:</div><div>
<br /></div><div><div></div><blockquote><div>Recently there has been a lot of interest in “ensemble learning” — methods that generate many classiﬁers and aggregate their results. Two well-known methods are boosting (see, e.g., Shapire et al., 1998) and bagging (Breiman, 1996) of classiﬁcation trees. In boosting, successive trees give extra weight to points incorrectly predicted by earlier predictors. In the end, a weighted vote is taken for prediction. In bagging, successive trees do not depend on earlier trees— each is independently constructed using a bootstrap sample of the data set. In the end, a simple majority vote is taken for prediction.</div></blockquote><div></div></div>